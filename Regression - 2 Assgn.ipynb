{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a18bf1e-f512-4efc-8679-ce013ab98c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.\n",
    "\n",
    "# R-squared measures how well a linear regression model fits the data.\n",
    "# It indicates the proportion of the variance in the dependent varibale explained by independent variables.\n",
    "\n",
    "# Formula: R-squared = 1 - (sum(yi - yhat)^2 / sum(yi - mean(yi))^2) \n",
    "# In this formula, n = number of data points\n",
    "# yi = actual observed value of the dependent variable\n",
    "# yhat = predicted value of the dependent variable by the regression model\n",
    "# mean(yi) = mean of th observed values.\n",
    "\n",
    "# R-squared ranges from 0 to 1, with higher values indicating better fit.\n",
    "# It's a common metric for assessing model goodness of fit.\n",
    "# However, a high R-sqaured does not guarantee a good model, and a low R-squared does not necessarily mean a bad model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6cd846a-9027-4ba1-acff-20f38e235cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.\n",
    "\n",
    "# Adjusted R-squared addresses some of the limitations of R-squared:\n",
    "\n",
    "# Regular R-squared:\n",
    "# Measures the proportion of variance in the dependent variables explained by the independent variables.\n",
    "# Increases as more predictors are added, even if they don't improve the model.\n",
    "# Doesn't account for overfitting.\n",
    "\n",
    "# Adjusted R-squared:\n",
    "# Penalizes excessive predictors by adjusting for model complexity.\n",
    "# Increases only if additional predictors improve the model significantly.\n",
    "# Provides a more accurate measure of model fit, especially when dealing with multiple predictors.\n",
    "# It is always lower or equal to the regualr R-squared.\n",
    "\n",
    "#Formula:\n",
    "\n",
    "# Adjusted R-squared = 1 - ((1 - R^2) * (n-1)/ (n-p-1))\n",
    "\n",
    "# n is the number of data points\n",
    "# p is the number of independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20cbe2e6-4ee1-48e5-9e0d-0e402c4afc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.\n",
    "\n",
    "# Adjusted R-squared is more appropriate in the following situations:\n",
    "\n",
    "# Multiple Predictors: When our regression model includes multiple predictors (independent variables).\n",
    "# Model Selection: To compare models with different numbers of predictors.\n",
    "# Avoiding Overfitting: When we wnat to penalize the inclusion of unnecessary predictors.\n",
    "# Model Assessment: For a more accurate evaluation of the model's goodness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af7933d-d886-4d2e-9243-0b12e68ddbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4.\n",
    "\n",
    "# RMSE:\n",
    "# Formula: sqrt(sum(actual - predicted)^2 / n)\n",
    "# Measures the square root of the average squared differencces between actual and predicted values.\n",
    "# Penalizes larger errors more than MSE.\n",
    "# Sensitive to outliers.\n",
    "\n",
    "# MSE:\n",
    "# Formula: sum(actual - predicted)^2 / n\n",
    "# Measures the avergae squared differences between actual and predicted values.\n",
    "# Emphasizes larger errors.\n",
    "# Sensitive to outliers.\n",
    "\n",
    "# MAE:\n",
    "# Formula: sum(|actual - predicted|) / n\n",
    "# Measures the average absolute differences between actual and predicted values.\n",
    "# Treats all errors equally.\n",
    "# Less sensitive to outliers compared to RMSE and MSE.\n",
    "\n",
    "# These metrics quantify the accuracy and goodness of fit of regression models, with RMSE and MSE giving more weight to larger errors, while MAE treats all errors equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bf5db24-8659-45dd-b81e-51394d59bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5.\n",
    "\n",
    "# Advantages:\n",
    "\n",
    "#RMSE:\n",
    "# Sensitive to large errors, making it useful for identifying significant deviations.\n",
    "# Provides a clear measure of error magnitude.\n",
    "\n",
    "#MSE:\n",
    "# Provides a continuous and differentiable loss function, making it suitable for optimization algorithms.\n",
    "# Penalizes larger errors more than smaller ones.\n",
    "# It has one local and one global minima\n",
    "\n",
    "# MAE:\n",
    "# Robust to outliers\n",
    "# Easier to understand since it's the average absolute error.\n",
    "\n",
    "# Disadvantages:\n",
    "\n",
    "# RMSE:\n",
    "# Sensitive to outliers and large errors, which can dominate the evaluation.\n",
    "\n",
    "#MSE:\n",
    "# Emphasizes larger errors and may not be suitable if smaller errors are more important.\n",
    "\n",
    "# MAE:\n",
    "# Ignores the magnitude of errors and lacks the squared term to differentiate between larger and smaller errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb540cfa-eb02-43b7-a6c4-804dde477872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6.\n",
    "\n",
    "# Lasso Regularization:\n",
    "# Concept: Lasso adds an L1 penalty to the linear/multiple regression objective, encouraging sparsity.\n",
    "# Difference from Ridge: Lasso uses absolute coefficients, potentially setting some to zero, unlike Ridge that uses the squared values of coefficients which is capable of shrinking coefficients towards zero without excluding any.\n",
    "# When to use Lasso: For feature selection and simplifying the model by excluding irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f6d55d4-a37d-46bb-9d6e-706ff7428af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7.\n",
    "\n",
    "# Regularized Linear Models for Overfitting:\n",
    "\n",
    "# Concept: Regularization techniques such as Lasso or Ridge add penalty terms to the linear/multiple regression objective to prevent overfitting.\n",
    "# Example: In a polynomial regression, when the degree is high, the model can overfit the training data. Regularization helps reduce the complexity of the model by shrinking coefficients, preventing extreme curve-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "281c32e3-24e8-42bb-8519-cbb9dbaf161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8.\n",
    "\n",
    "# Limitations of Regularized Linear Models:\n",
    "\n",
    "# Sensitivity to Hyperparameters: The choice of regularization strength (e.g. alpha value in Ridge/Lasso) can be critical and is often data-dependent.\n",
    "# Feature Selection: Lasso regularization encourages feature selection, which may not be suitable when all features are informative.\n",
    "# Assumption of Linearity: Regularized linear models assume a linear relationship b/w features and target, which may not hold in some real-world scenarios.\n",
    "# Limited for Nonlinear Data: They may not capture complex nonlinear relationships b/w features and the target variable.\n",
    "# Interpretability: Regularized models make coefficient values shrink towards zero, which can make interpretation more challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4abccf8b-cc2f-4f3a-a978-0cb208a09869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9.\n",
    "\n",
    "# Choice of Better Model: Model B with an MAE of 8 would be chosen as the better performer because it has a lower error compared to Model A with an RMSE of 10.\n",
    "\n",
    "# Advantages of MAE: MAE is robust to outliers and gives equal weight to all errors, making it easier to interpret.\n",
    "\n",
    "# Limitations of Metric Choice: While MAE is suitable in this scenario, it doesn't consider the magnitude of errors and may not be sensitive to large errors in some cases. The choice of metric should align with the specific objectives and characteristics of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a1bd23b-963e-4bed-9b1f-8901cd544fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10.\n",
    "\n",
    "# Choice of Better Model: The choice depends on the specific goals and characteristics of the problem.\n",
    "\n",
    "# Model A (Ridge Regularization, lambda=0.1): Suitable when multicollinearity is an issue, and we want to shrink coefficients. It doesn't perform feature selection.\n",
    "\n",
    "# Model B (Lasso Regularization, lambda=0.5): Useful when we want feature selection as it tends to set some coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "# Trade-offs: Ridge is better at handling multicollinearity but doesn't perform feature selection. Lasso can perform feature selection but might not be as effective with strong multicollinearity.\n",
    "\n",
    "# Limitations: The choice of regularization depends on the balance between feature selection, handling multicollinearity, and the problem's specifics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3548cb38-0ccd-4fdd-baee-c685fb43f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e826c8-d6ca-4f0d-acf2-34ab25de701d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
